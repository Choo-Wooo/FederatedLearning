{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 292
    },
    "id": "vkZxat4Y-IsQ",
    "outputId": "da86392c-66e8-4b60-b471-086e745cdcbc"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import os\n",
    "import random\n",
    "from torch.autograd import Variable\n",
    "import copy\n",
    "from torch import nn, optim\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "from collections import OrderedDict\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import csv\n",
    "import time\n",
    "import math\n",
    "import re\n",
    "import json\n",
    "import sys\n",
    "\n",
    "sys.path.append('./utils/')\n",
    "from model import *\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## If you want to experiment with a different seed value, change 'trial_times'\n",
    "trial_times = 1\n",
    "SEED = 42 + trial_times -1\n",
    "fix_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "O0TfzOhU-QlG"
   },
   "outputs": [],
   "source": [
    "class Argments():\n",
    "  def __init__(self):\n",
    "    self.batch_size = 20 \n",
    "    self.test_batch = 1000\n",
    "    self.global_epochs = 200\n",
    "    self.local_epochs = 2\n",
    "    self.lr = 10**(-3)\n",
    "    self.momentum = 0.9\n",
    "    self.weight_decay = 10**-4.0\n",
    "    self.clip = 20.0\n",
    "    self.partience = 10\n",
    "    self.worker_num = 20\n",
    "    self.participation_rate = 1\n",
    "    self.sample_num = int(self.worker_num * self.participation_rate)\n",
    "    self.total_data_rate = 1\n",
    "    self.unlabeleddata_size = 1000\n",
    "    self.device = device = torch.device('cuda:0'if torch.cuda.is_available() else'cpu')\n",
    "    self.criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    ## If you use MNIST or CIFAR-10, the degree of data heterogeneity can be changed by changing alpha_label and alpha_size.\n",
    "    self.alpha_label = 0.5\n",
    "    self.alpha_size = 10\n",
    "    \n",
    "    ## Select a dataset from 'FEMNIST','Shakespeare','Sent140','MNIST', or 'CIFAR-10'.\n",
    "    self.dataset_name = 'FEMNIST'\n",
    "\n",
    "\n",
    "args = Argments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.dataset_name=='FEMNIST':\n",
    "    from femnist_dataset import *\n",
    "    args.num_classes = 62\n",
    "    model_name = \"CNN(num_classes=args.num_classes)\"\n",
    "    \n",
    "elif args.dataset_name=='Shakespeare':\n",
    "    from shakespeare_dataset import *\n",
    "    model_name = \"RNN()\"\n",
    "    \n",
    "elif args.dataset_name=='Sent140':\n",
    "    from sent140_dataset import *\n",
    "    from utils_sent140 import *\n",
    "    VOCAB_DIR = '../models/embs.json'\n",
    "    _, indd, vocab = get_word_emb_arr(VOCAB_DIR)\n",
    "    model_name = \"RNNSent(args,'LSTM', 2, 25, 128, 1, 0.5, tie_weights=False)\"\n",
    "    \n",
    "elif args.dataset_name=='MNIST':\n",
    "    from mnist_dataset import *\n",
    "    args.num_classes = 10\n",
    "    model_name = \"CNN(num_classes=args.num_classes)\"\n",
    "    \n",
    "elif args.dataset_name=='CIFAR-10':\n",
    "    from cifar10_dataset import *\n",
    "    model_name = \"vgg13()\"\n",
    "    \n",
    "else:\n",
    "    print('Error: The name of the dataset is incorrect. Please re-set the \"dataset_name\".')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2619, 456, 102, 3037, 1126, 1003, 914, 571, 3016, 419, 2771, 3033, 2233, 356, 2418, 1728, 130, 122, 383, 895]\n"
     ]
    }
   ],
   "source": [
    "federated_trainset,federated_valset,federated_testset,global_trainloader,global_valloader,global_testloader,unlabeled_dataset = get_dataset(args, Centralized=True,unlabeled_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "Yu90X1TWJVKJ"
   },
   "outputs": [],
   "source": [
    "class Server():\n",
    "  def __init__(self):\n",
    "    self.model = eval(model_name)\n",
    "\n",
    "  def create_worker(self,federated_trainset,federated_valset,federated_testset):\n",
    "    workers = []\n",
    "    for i in range(args.worker_num):\n",
    "      workers.append(Worker(federated_trainset[i],federated_valset[i],federated_testset[i]))\n",
    "    return workers\n",
    "\n",
    "  def sample_worker(self,workers):\n",
    "    sample_worker = []\n",
    "    sample_worker_num = random.sample(range(args.worker_num),args.sample_num)\n",
    "    for i in sample_worker_num:\n",
    "      sample_worker.append(workers[i])\n",
    "    return sample_worker\n",
    "\n",
    "\n",
    "  def send_model(self,workers):\n",
    "    nums = 0\n",
    "    for worker in workers:\n",
    "      nums += worker.train_data_num\n",
    "\n",
    "    for worker in workers:\n",
    "      worker.aggregation_weight = 1.0*worker.train_data_num/nums\n",
    "      worker.model = copy.deepcopy(self.model)\n",
    "      worker.model = worker.model.to(args.device)\n",
    "\n",
    "  def aggregate_model(self,workers):   \n",
    "    new_params = OrderedDict()\n",
    "    for i,worker in enumerate(workers):\n",
    "      worker_state = worker.model.state_dict()\n",
    "      for key in worker_state.keys():\n",
    "        if i==0:\n",
    "          new_params[key] = worker_state[key]*worker.aggregation_weight\n",
    "        else:\n",
    "          new_params[key] += worker_state[key]*worker.aggregation_weight\n",
    "      worker.model = worker.model.to('cpu')\n",
    "      del worker.model\n",
    "    self.model.load_state_dict(new_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "LDWEBjgfJYFc"
   },
   "outputs": [],
   "source": [
    "class Worker():\n",
    "  def __init__(self,trainset,valset,testset):\n",
    "    self.trainloader = torch.utils.data.DataLoader(trainset,batch_size=args.batch_size,shuffle=True,num_workers=2)\n",
    "    self.valloader = torch.utils.data.DataLoader(valset,batch_size=args.test_batch,shuffle=False,num_workers=2)\n",
    "    self.testloader = torch.utils.data.DataLoader(testset,batch_size=args.test_batch,shuffle=False,num_workers=2)\n",
    "    self.model = None\n",
    "    self.train_data_num = len(trainset)\n",
    "    self.test_data_num = len(testset)\n",
    "    self.aggregation_weight = None\n",
    "\n",
    "  def local_train(self):\n",
    "    acc_train,loss_train = local_train(self.model,args.criterion,self.trainloader,args.local_epochs)\n",
    "    acc_valid,loss_valid = test(self.model,args.criterion,self.valloader)\n",
    "    return acc_train,loss_train,acc_valid,loss_valid\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def local_train(model,criterion,trainloader,epochs):\n",
    "  if args.dataset_name=='Sent140':\n",
    "      model.train()\n",
    "      hidden_train = model.init_hidden(args.batch_size)\n",
    "      optimizer = optim.SGD(model.parameters(),lr=args.lr,momentum=args.momentum,weight_decay=args.weight_decay)\n",
    "      for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        count = 0\n",
    "        for (data,labels) in trainloader:\n",
    "          data, labels = process_x(data, indd), process_y(labels, indd)\n",
    "          if args.batch_size != 1 and data.shape[0] != args.batch_size:\n",
    "            break\n",
    "          data,labels = torch.from_numpy(data).to(args.device), torch.from_numpy(labels).to(args.device)\n",
    "          optimizer.zero_grad()\n",
    "          hidden_train = repackage_hidden(hidden_train)\n",
    "          outputs, hidden_train = model(data, hidden_train) \n",
    "          loss = criterion(outputs.t(), torch.max(labels, 1)[1])\n",
    "          running_loss += loss.item()\n",
    "          _, predicted = torch.max(outputs.t(), 1)\n",
    "          correct += (predicted == torch.max(labels, 1)[1]).sum().item()\n",
    "          count += len(labels)\n",
    "          loss.backward()\n",
    "          torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip)\n",
    "          optimizer.step()\n",
    "\n",
    "      return 100.0*correct/count,running_loss/len(trainloader)\n",
    "\n",
    "  else:\n",
    "      optimizer = optim.SGD(model.parameters(),lr=args.lr,momentum=args.momentum,weight_decay=args.weight_decay)\n",
    "      model.train()\n",
    "      for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        count = 0\n",
    "        for (data,labels) in trainloader:\n",
    "          data,labels = Variable(data),Variable(labels)\n",
    "          data,labels = data.to(args.device),labels.to(args.device)\n",
    "          optimizer.zero_grad()\n",
    "          outputs = model(data)\n",
    "          loss = criterion(outputs,labels)\n",
    "          running_loss += loss.item()\n",
    "          predicted = torch.argmax(outputs,dim=1)\n",
    "          correct += (predicted==labels).sum().item()\n",
    "          count += len(labels)\n",
    "          loss.backward()\n",
    "          torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip)\n",
    "          optimizer.step()\n",
    "\n",
    "      return 100.0*correct/count,running_loss/len(trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def global_train(model,criterion,trainloader,valloader,epochs,partience=0,early_stop=False):\n",
    "  if args.dataset_name=='Sent140':\n",
    "      if early_stop:\n",
    "        early_stopping = Early_Stopping(partience)\n",
    "\n",
    "      acc_train = []\n",
    "      loss_train = []\n",
    "      acc_valid = []\n",
    "      loss_valid = []\n",
    "      optimizer = optim.SGD(model.parameters(),lr=args.lr,momentum=args.momentum,weight_decay=args.weight_decay)\n",
    "      hidden_train = model.init_hidden(args.batch_size)\n",
    "      for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        count = 0\n",
    "        model.train()\n",
    "        for (data,labels) in trainloader:\n",
    "          data, labels = process_x(data, indd), process_y(labels, indd)\n",
    "          if args.batch_size != 1 and data.shape[0] != args.batch_size:\n",
    "            break\n",
    "          data,labels = torch.from_numpy(data).to(args.device), torch.from_numpy(labels).to(args.device)\n",
    "          optimizer.zero_grad()\n",
    "          hidden_train = repackage_hidden(hidden_train)\n",
    "          outputs, hidden_train = model(data, hidden_train) \n",
    "          loss = criterion(outputs.t(), torch.max(labels, 1)[1])\n",
    "          running_loss += loss.item()\n",
    "          _, predicted = torch.max(outputs.t(), 1)\n",
    "          correct += (predicted == torch.max(labels, 1)[1]).sum().item()\n",
    "          count += len(labels)\n",
    "          loss.backward()\n",
    "          torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip)\n",
    "          optimizer.step()\n",
    "        acc_train.append(100.0*correct/count)\n",
    "        loss_train.append(running_loss/len(trainloader))\n",
    "\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        count = 0\n",
    "        model.eval()\n",
    "        hidden_test = model.init_hidden(args.test_batch)\n",
    "        for (data,labels) in valloader:\n",
    "          data, labels = process_x(data, indd), process_y(labels, indd)\n",
    "          if args.test_batch != 1 and data.shape[0] != args.test_batch:\n",
    "            break\n",
    "          data,labels = torch.from_numpy(data).to(args.device), torch.from_numpy(labels).to(args.device)\n",
    "          hidden_test = repackage_hidden(hidden_test)\n",
    "          outputs, hidden_test = model(data, hidden_test) \n",
    "          running_loss += criterion(outputs.t(), torch.max(labels, 1)[1]).item()\n",
    "          _, predicted = torch.max(outputs.t(), 1)\n",
    "          correct += (predicted == torch.max(labels, 1)[1]).sum().item()\n",
    "          count += len(labels)\n",
    "\n",
    "        print('Epoch:{}  accuracy:{}  loss:{}'.format(epoch+1,100.0*correct/count,running_loss/len(valloader)))\n",
    "        acc_valid.append(100.0*correct/count)\n",
    "        loss_valid.append(running_loss/len(valloader))\n",
    "        if early_stop:\n",
    "          if early_stopping.validate(running_loss):\n",
    "            print('Early Stop')\n",
    "            return acc_train,loss_train,acc_valid,loss_valid\n",
    "\n",
    "      return acc_train,loss_train,acc_valid,loss_valid\n",
    "\n",
    "  else:\n",
    "      if early_stop:\n",
    "        early_stopping = Early_Stopping(partience)\n",
    "\n",
    "      acc_train = []\n",
    "      loss_train = []\n",
    "      acc_valid = []\n",
    "      loss_valid = []\n",
    "      optimizer = optim.SGD(model.parameters(),lr=args.lr,momentum=args.momentum,weight_decay=args.weight_decay)\n",
    "      for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        count = 0\n",
    "        model.train()\n",
    "        for (data,labels) in trainloader:\n",
    "          count += len(labels)\n",
    "          data,labels = Variable(data),Variable(labels)\n",
    "          data,labels = data.to(args.device),labels.to(args.device)\n",
    "          optimizer.zero_grad()\n",
    "          outputs = model(data)\n",
    "          loss = criterion(outputs,labels)\n",
    "          running_loss += loss.item()\n",
    "          predicted = torch.argmax(outputs,dim=1)\n",
    "          correct += (predicted==labels).sum().item()\n",
    "          loss.backward()\n",
    "          torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip)\n",
    "          optimizer.step()\n",
    "        acc_train.append(100.0*correct/count)\n",
    "        loss_train.append(running_loss/len(trainloader))\n",
    "\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        count = 0\n",
    "        model.eval()\n",
    "        for (data,labels) in valloader:\n",
    "          count += len(labels)\n",
    "          data,labels = data.to(args.device),labels.to(args.device)\n",
    "          outputs = model(data)\n",
    "          loss = criterion(outputs,labels)\n",
    "          running_loss += loss.item()\n",
    "          predicted = torch.argmax(outputs,dim=1)\n",
    "          correct += (predicted==labels).sum().item()\n",
    "\n",
    "        print('Epoch:{}  accuracy:{}  loss:{}'.format(epoch+1,100.0*correct/count,running_loss/len(valloader)))\n",
    "        acc_valid.append(100.0*correct/count)\n",
    "        loss_valid.append(running_loss/len(valloader))\n",
    "        if early_stop:\n",
    "          if early_stopping.validate(running_loss):\n",
    "            print('Early Stop')\n",
    "            return acc_train,loss_train,acc_valid,loss_valid\n",
    "\n",
    "      return acc_train,loss_train,acc_valid,loss_valid    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "oA4URv9mQ3xV"
   },
   "outputs": [],
   "source": [
    "def test(model,criterion,testloader):\n",
    "  if args.dataset_name=='Sent140':\n",
    "      model.eval()\n",
    "      hidden_test = model.init_hidden(args.test_batch)\n",
    "      running_loss = 0.0\n",
    "      correct = 0\n",
    "      count = 0\n",
    "      for (data,labels) in testloader:\n",
    "        data, labels = process_x(data, indd), process_y(labels, indd)\n",
    "        if args.test_batch != 1 and data.shape[0] != args.test_batch:\n",
    "          break\n",
    "        data,labels = torch.from_numpy(data).to(args.device), torch.from_numpy(labels).to(args.device)\n",
    "        hidden_test = repackage_hidden(hidden_test)\n",
    "        outputs, hidden_test = model(data, hidden_test) \n",
    "        running_loss += criterion(outputs.t(), torch.max(labels, 1)[1]).item()\n",
    "        _, predicted = torch.max(outputs.t(), 1)\n",
    "        correct += (predicted == torch.max(labels, 1)[1]).sum().item()\n",
    "        count += len(labels)\n",
    "\n",
    "      accuracy = 100.0*correct/count\n",
    "      loss = running_loss/len(testloader)\n",
    "\n",
    "\n",
    "      return accuracy,loss\n",
    "  \n",
    "  else:\n",
    "      model.eval()\n",
    "      running_loss = 0.0\n",
    "      correct = 0\n",
    "      count = 0\n",
    "      for (data,labels) in testloader:\n",
    "        data,labels = data.to(args.device),labels.to(args.device)\n",
    "        outputs = model(data)\n",
    "        running_loss += criterion(outputs,labels).item()\n",
    "        predicted = torch.argmax(outputs,dim=1)\n",
    "        correct += (predicted==labels).sum().item()\n",
    "        count += len(labels)\n",
    "\n",
    "      accuracy = 100.0*correct/count\n",
    "      loss = running_loss/len(testloader)\n",
    "\n",
    "\n",
    "      return accuracy,loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1  accuracy:4.363376251788269  loss:3.791600465774536\n",
      "Epoch:2  accuracy:3.4334763948497855  loss:3.7835288047790527\n",
      "Epoch:3  accuracy:6.294706723891273  loss:3.7503710985183716\n",
      "Epoch:4  accuracy:6.080114449213162  loss:3.739477038383484\n",
      "Epoch:5  accuracy:7.7968526466380546  loss:3.688174605369568\n",
      "Epoch:6  accuracy:12.875536480686696  loss:3.6055145263671875\n",
      "Epoch:7  accuracy:17.954220314735338  loss:3.4536060094833374\n",
      "Epoch:8  accuracy:26.46638054363376  loss:3.1851083040237427\n",
      "Epoch:9  accuracy:34.83547925608011  loss:2.8780847787857056\n",
      "Epoch:10  accuracy:40.128755364806864  loss:2.598298668861389\n",
      "Epoch:11  accuracy:46.49499284692418  loss:2.3311718702316284\n",
      "Epoch:12  accuracy:51.502145922746784  loss:2.0877662301063538\n",
      "Epoch:13  accuracy:55.0071530758226  loss:1.8827438950538635\n",
      "Epoch:14  accuracy:58.15450643776824  loss:1.7601023316383362\n",
      "Epoch:15  accuracy:59.65665236051502  loss:1.623025894165039\n",
      "Epoch:16  accuracy:61.23032904148784  loss:1.5187321305274963\n",
      "Epoch:17  accuracy:64.8068669527897  loss:1.4039602875709534\n",
      "Epoch:18  accuracy:66.02288984263234  loss:1.340105950832367\n",
      "Epoch:19  accuracy:66.02288984263234  loss:1.2881104946136475\n",
      "Epoch:20  accuracy:67.73962804005723  loss:1.2061210870742798\n",
      "Epoch:21  accuracy:67.45350500715307  loss:1.1657249331474304\n",
      "Epoch:22  accuracy:69.5994277539342  loss:1.0986761450767517\n",
      "Epoch:23  accuracy:71.03004291845494  loss:1.0591682195663452\n",
      "Epoch:24  accuracy:71.81688125894135  loss:1.0296665728092194\n",
      "Epoch:25  accuracy:72.3175965665236  loss:0.9756879806518555\n",
      "Epoch:26  accuracy:73.67668097281832  loss:0.9543106853961945\n",
      "Epoch:27  accuracy:73.4620886981402  loss:0.9307592213153839\n",
      "Epoch:28  accuracy:75.25035765379113  loss:0.9032202661037445\n",
      "Epoch:29  accuracy:73.96280400572246  loss:0.8861303627490997\n",
      "Epoch:30  accuracy:75.10729613733906  loss:0.8809565007686615\n",
      "Epoch:31  accuracy:75.60801144492132  loss:0.8645032346248627\n",
      "Epoch:32  accuracy:75.3934191702432  loss:0.8538592159748077\n",
      "Epoch:33  accuracy:75.46494992846924  loss:0.8544571697711945\n",
      "Epoch:34  accuracy:76.89556509298998  loss:0.8193628489971161\n",
      "Epoch:35  accuracy:76.89556509298998  loss:0.805802971124649\n",
      "Epoch:36  accuracy:77.25321888412017  loss:0.8084817230701447\n",
      "Epoch:37  accuracy:77.25321888412017  loss:0.7895021140575409\n",
      "Epoch:38  accuracy:76.82403433476395  loss:0.7964694797992706\n",
      "Epoch:39  accuracy:76.89556509298998  loss:0.8140785992145538\n",
      "Epoch:40  accuracy:78.04005722460658  loss:0.7640616595745087\n",
      "Epoch:41  accuracy:77.6824034334764  loss:0.7986332476139069\n",
      "Epoch:42  accuracy:77.6824034334764  loss:0.7822713255882263\n",
      "Epoch:43  accuracy:78.32618025751073  loss:0.7826590240001678\n",
      "Epoch:44  accuracy:77.3247496423462  loss:0.7772925198078156\n",
      "Epoch:45  accuracy:78.39771101573676  loss:0.7870384156703949\n",
      "Epoch:46  accuracy:78.96995708154506  loss:0.7602802813053131\n",
      "Epoch:47  accuracy:76.96709585121603  loss:0.819312185049057\n",
      "Epoch:48  accuracy:79.11301859799714  loss:0.7520691752433777\n",
      "Epoch:49  accuracy:79.25608011444922  loss:0.7419840693473816\n",
      "Epoch:50  accuracy:78.54077253218884  loss:0.7686368525028229\n",
      "Epoch:51  accuracy:78.32618025751073  loss:0.78046914935112\n",
      "Epoch:52  accuracy:78.75536480686695  loss:0.7782151401042938\n",
      "Epoch:53  accuracy:78.826895565093  loss:0.7897739112377167\n",
      "Epoch:54  accuracy:78.75536480686695  loss:0.7948503792285919\n",
      "Epoch:55  accuracy:78.826895565093  loss:0.7779313623905182\n",
      "Epoch:56  accuracy:77.46781115879828  loss:0.7915183305740356\n",
      "Epoch:57  accuracy:78.32618025751073  loss:0.7611181139945984\n",
      "Epoch:58  accuracy:78.68383404864092  loss:0.7532977759838104\n",
      "Epoch:59  accuracy:79.39914163090128  loss:0.7536207437515259\n",
      "Epoch:60  accuracy:79.8283261802575  loss:0.7378535270690918\n",
      "Epoch:61  accuracy:79.97138769670958  loss:0.752638965845108\n",
      "Epoch:62  accuracy:78.2546494992847  loss:0.7932710945606232\n",
      "Epoch:63  accuracy:79.68526466380544  loss:0.7888512015342712\n",
      "Epoch:64  accuracy:79.04148783977111  loss:0.7800142168998718\n",
      "Epoch:65  accuracy:77.82546494992847  loss:0.7643765211105347\n",
      "Epoch:66  accuracy:78.826895565093  loss:0.7699918746948242\n",
      "Epoch:67  accuracy:79.32761087267525  loss:0.7682724297046661\n",
      "Epoch:68  accuracy:79.75679542203147  loss:0.7822937965393066\n",
      "Epoch:69  accuracy:79.54220314735336  loss:0.7919928431510925\n",
      "Epoch:70  accuracy:79.8283261802575  loss:0.7827517986297607\n",
      "Epoch:71  accuracy:79.6137339055794  loss:0.7931791543960571\n",
      "Early Stop\n"
     ]
    }
   ],
   "source": [
    "model = eval(model_name)\n",
    "model = model.to(args.device)\n",
    "\n",
    "start = time.time()\n",
    "acc_train,loss_train,acc_valid,loss_valid = global_train(model,args.criterion,global_trainloader,global_valloader,args.global_epochs,partience=args.partience,early_stop=True)\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train time：243.52622294425964[s]\n"
     ]
    }
   ],
   "source": [
    "print('train time：{}[s]'.format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "server = Server()\n",
    "workers = server.create_worker(federated_trainset,federated_valset,federated_testset)\n",
    "server.model = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "mi_uceyoptLP",
    "outputId": "bc067e09-01bc-4e65-daf9-ac2f42373cbd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worker1 accuracy:76.47058823529412  loss:0.9733505249023438\n",
      "Worker2 accuracy:88.57142857142857  loss:0.25058314204216003\n",
      "Worker3 accuracy:80.0  loss:0.7634997963905334\n",
      "Worker4 accuracy:66.66666666666667  loss:1.3587229251861572\n",
      "Worker5 accuracy:76.47058823529412  loss:0.7209831476211548\n",
      "Worker6 accuracy:80.95238095238095  loss:0.5895506739616394\n",
      "Worker7 accuracy:78.94736842105263  loss:0.8862633109092712\n",
      "Worker8 accuracy:87.5  loss:0.3322001099586487\n",
      "Worker9 accuracy:47.36842105263158  loss:1.3017655611038208\n",
      "Worker10 accuracy:80.48780487804878  loss:0.6653041839599609\n",
      "Worker11 accuracy:72.22222222222223  loss:0.7749717235565186\n",
      "Worker12 accuracy:84.21052631578948  loss:0.6506400108337402\n",
      "Worker13 accuracy:84.21052631578948  loss:0.8365836143493652\n",
      "Worker14 accuracy:66.66666666666667  loss:1.7973052263259888\n",
      "Worker15 accuracy:73.6842105263158  loss:0.7650867104530334\n",
      "Worker16 accuracy:76.47058823529412  loss:0.4999558925628662\n",
      "Worker17 accuracy:75.75757575757575  loss:1.5940836668014526\n",
      "Worker18 accuracy:80.64516129032258  loss:0.9264622330665588\n",
      "Worker19 accuracy:85.29411764705883  loss:0.47791895270347595\n",
      "Worker20 accuracy:77.77777777777777  loss:0.9664749503135681\n",
      "Test  loss:0.8565853178501129  accuracy:77.01873098838051\n"
     ]
    }
   ],
   "source": [
    "acc_test = []\n",
    "loss_test = []\n",
    "\n",
    "server.model.to(args.device)\n",
    "\n",
    "nums = 0\n",
    "for worker in workers:\n",
    "  nums += worker.test_data_num\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for i,worker in enumerate(workers):\n",
    "  worker.aggregation_weight = 1.0*worker.test_data_num/nums\n",
    "  acc_tmp,loss_tmp = test(server.model,args.criterion,worker.testloader)\n",
    "  acc_test.append(acc_tmp)\n",
    "  loss_test.append(loss_tmp)\n",
    "  print('Worker{} accuracy:{}  loss:{}'.format(i+1,acc_tmp,loss_tmp))\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "acc_test_avg = sum(acc_test)/len(acc_test)\n",
    "loss_test_avg = sum(loss_test)/len(loss_test)\n",
    "print('Test  loss:{}  accuracy:{}'.format(loss_test_avg,acc_test_avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worker1 Valid accuracy:88.37209302325581  loss:0.4103790521621704\n",
      "Worker1 Test accuracy:88.23529411764706  loss:0.8791719079017639\n",
      "Worker2 Valid accuracy:87.23404255319149  loss:0.5206003189086914\n",
      "Worker2 Test accuracy:88.57142857142857  loss:0.2323041409254074\n",
      "Worker3 Valid accuracy:80.0  loss:0.7459836006164551\n",
      "Worker3 Test accuracy:82.5  loss:0.7393306493759155\n",
      "Worker4 Valid accuracy:86.95652173913044  loss:0.5901270508766174\n",
      "Worker4 Test accuracy:72.22222222222223  loss:1.3095393180847168\n",
      "Worker5 Valid accuracy:84.61538461538461  loss:0.4709709584712982\n",
      "Worker5 Test accuracy:85.29411764705883  loss:0.5873681902885437\n",
      "Worker6 Valid accuracy:78.18181818181819  loss:0.8970992565155029\n",
      "Worker6 Test accuracy:80.95238095238095  loss:0.6027767658233643\n",
      "Worker7 Valid accuracy:83.0  loss:0.624214768409729\n",
      "Worker7 Test accuracy:76.3157894736842  loss:1.0254863500595093\n",
      "Worker8 Valid accuracy:82.3529411764706  loss:0.6872109174728394\n",
      "Worker8 Test accuracy:84.375  loss:0.329889714717865\n",
      "Worker9 Valid accuracy:75.51020408163265  loss:0.6752806901931763\n",
      "Worker9 Test accuracy:52.63157894736842  loss:1.0803722143173218\n",
      "Worker10 Valid accuracy:84.40366972477064  loss:0.41518616676330566\n",
      "Worker10 Test accuracy:87.8048780487805  loss:0.4874541163444519\n",
      "Worker11 Valid accuracy:73.91304347826087  loss:1.2006819248199463\n",
      "Worker11 Test accuracy:77.77777777777777  loss:0.6837190985679626\n",
      "Worker12 Valid accuracy:81.63265306122449  loss:0.5644474625587463\n",
      "Worker12 Test accuracy:73.6842105263158  loss:0.597819447517395\n",
      "Worker13 Valid accuracy:75.51020408163265  loss:1.0338342189788818\n",
      "Worker13 Test accuracy:84.21052631578948  loss:0.7800086140632629\n",
      "Worker14 Valid accuracy:79.48717948717949  loss:1.0009363889694214\n",
      "Worker14 Test accuracy:63.333333333333336  loss:1.849562406539917\n",
      "Worker15 Valid accuracy:85.71428571428571  loss:0.6850737929344177\n",
      "Worker15 Test accuracy:78.94736842105263  loss:0.77125483751297\n",
      "Worker16 Valid accuracy:84.44444444444444  loss:0.5111790299415588\n",
      "Worker16 Test accuracy:82.3529411764706  loss:0.5474638938903809\n",
      "Worker17 Valid accuracy:72.72727272727273  loss:1.074870228767395\n",
      "Worker17 Test accuracy:75.75757575757575  loss:1.5621083974838257\n",
      "Worker18 Valid accuracy:82.92682926829268  loss:1.0092014074325562\n",
      "Worker18 Test accuracy:77.41935483870968  loss:0.935575008392334\n",
      "Worker19 Valid accuracy:87.77777777777777  loss:0.421353280544281\n",
      "Worker19 Test accuracy:94.11764705882354  loss:0.33747097849845886\n",
      "Worker20 Valid accuracy:71.11111111111111  loss:1.0408517122268677\n",
      "Worker20 Test accuracy:77.77777777777777  loss:0.9244892597198486\n",
      "Validation(tune)  loss:0.7289741113781929  accuracy:81.29357381235683\n",
      "Test(tune)  loss:0.8131582655012608  accuracy:79.21406014820987\n"
     ]
    }
   ],
   "source": [
    "acc_tune_test = []\n",
    "loss_tune_test = []\n",
    "acc_tune_valid = []\n",
    "loss_tune_valid = []\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for i,worker in enumerate(workers):\n",
    "    worker.model = copy.deepcopy(server.model)\n",
    "    worker.model = worker.model.to(args.device)\n",
    "    _,_,acc_tmp,loss_tmp = worker.local_train()\n",
    "    acc_tune_valid.append(acc_tmp)\n",
    "    loss_tune_valid.append(loss_tmp)\n",
    "    print('Worker{} Valid accuracy:{}  loss:{}'.format(i+1,acc_tmp,loss_tmp))\n",
    "    \n",
    "    acc_tmp,loss_tmp = test(worker.model,args.criterion,worker.testloader)\n",
    "    acc_tune_test.append(acc_tmp)\n",
    "    loss_tune_test.append(loss_tmp)\n",
    "    print('Worker{} Test accuracy:{}  loss:{}'.format(i+1,acc_tmp,loss_tmp))\n",
    "    worker.model = worker.model.to('cpu')\n",
    "    del worker.model\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "acc_valid_avg = sum(acc_tune_valid)/len(acc_tune_valid)\n",
    "loss_valid_avg = sum(loss_tune_valid)/len(loss_tune_valid)\n",
    "print('Validation(tune)  loss:{}  accuracy:{}'.format(loss_valid_avg,acc_valid_avg))\n",
    "acc_test_avg = sum(acc_tune_test)/len(acc_tune_test)\n",
    "loss_test_avg = sum(loss_tune_test)/len(loss_tune_test)\n",
    "print('Test(tune)  loss:{}  accuracy:{}'.format(loss_test_avg,acc_test_avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'Centralized_{}'.format(args.dataset_name)\n",
    "result_path = '../result/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_train = pd.DataFrame(acc_train)\n",
    "loss_train = pd.DataFrame(loss_train)\n",
    "acc_valid = pd.DataFrame(acc_valid)\n",
    "loss_valid = pd.DataFrame(loss_valid)\n",
    "\n",
    "acc_test = pd.DataFrame(acc_test)\n",
    "loss_test = pd.DataFrame(loss_test)\n",
    "\n",
    "acc_tune_valid = pd.DataFrame(acc_tune_valid)\n",
    "loss_tune_valid = pd.DataFrame(loss_tune_valid)\n",
    "\n",
    "acc_tune_test = pd.DataFrame(acc_tune_test)\n",
    "loss_tune_test = pd.DataFrame(loss_tune_test)\n",
    "\n",
    "\n",
    "acc_train.to_csv(result_path+filename+'_train_acc.csv',index=False, header=False)\n",
    "loss_train.to_csv(result_path+filename+'_train_loss.csv',index=False, header=False)\n",
    "acc_valid.to_csv(result_path+filename+'_valid_acc.csv',index=False, header=False)\n",
    "loss_valid.to_csv(result_path+filename+'_valid_loss.csv',index=False, header=False)\n",
    "acc_test.to_csv(result_path+filename+'_test_acc.csv',index=False, header=False)\n",
    "loss_test.to_csv(result_path+filename+'_test_loss.csv',index=False, header=False)\n",
    "acc_tune_valid.to_csv(result_path+filename+'_fine-tune_valid_acc.csv',index=False, header=False)\n",
    "loss_tune_valid.to_csv(result_path+filename+'_fine-tune_valid_loss.csv',index=False, header=False)\n",
    "acc_tune_test.to_csv(result_path+filename+'_fine-tune_test_acc.csv',index=False, header=False)\n",
    "loss_tune_test.to_csv(result_path+filename+'_fine-tune_test_loss.csv',index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "FedAvg_femnist.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
